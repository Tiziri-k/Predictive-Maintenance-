{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import os\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from regressionLinear import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYSPARK_PYTHON: C:\\Users\\Slash\\anaconda3\\envs\\new_pyspark_env\\python.exe\n",
      "PYSPARK_DRIVER_PYTHON: C:\\Users\\Slash\\anaconda3\\envs\\new_pyspark_env\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# updater tjrs les path de python car ca ne marche pas manuallement\n",
    "os.environ['PYSPARK_PYTHON'] = r\"C:\\Users\\Slash\\anaconda3\\envs\\new_pyspark_env\\python.exe\"\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = r\"C:\\Users\\Slash\\anaconda3\\envs\\new_pyspark_env\\python.exe\"\n",
    "# check les paths\n",
    "print(\"PYSPARK_PYTHON:\", os.environ.get('PYSPARK_PYTHON'))\n",
    "print(\"PYSPARK_DRIVER_PYTHON:\", os.environ.get('PYSPARK_DRIVER_PYTHON'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lire la data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+-------+-------+-------+-------+--------+--------+--------+---+--------------+---------------------+---------------------+----------------------+---------------------+---------------------+---------------------+----------------------+----------------------+--------------------------+--------------------------+--------------------------+---------------------------+--------------------------+---------------------------+---------------------------+--------------------------+\n",
      "|engine|cycle_time|sensor2|sensor3|sensor4|sensor7|sensor9|sensor12|sensor14|sensor17|rul|classification|sensor9_rollingmean_4|sensor3_rollingmean_4|sensor17_rollingmean_4|sensor2_rollingmean_4|sensor4_rollingmean_4|sensor7_rollingmean_4|sensor14_rollingmean_4|sensor12_rollingmean_4|sensor7_rollingmean_4_norm|sensor3_rollingmean_4_norm|sensor2_rollingmean_4_norm|sensor17_rollingmean_4_norm|sensor9_rollingmean_4_norm|sensor14_rollingmean_4_norm|sensor12_rollingmean_4_norm|sensor4_rollingmean_4_norm|\n",
      "+------+----------+-------+-------+-------+-------+-------+--------+--------+--------+---+--------------+---------------------+---------------------+----------------------+---------------------+---------------------+---------------------+----------------------+----------------------+--------------------------+--------------------------+--------------------------+---------------------------+--------------------------+---------------------------+---------------------------+--------------------------+\n",
      "|     1|         1| 643.02|1585.29|1398.21|  553.9|9050.17|  521.72| 8125.55|   392.0| 15|             1|              9050.17|              1585.29|                 392.0|               643.02|              1398.21|                553.9|               8125.55|                521.72|       0.03070175438603402|        0.4571674558760084|                       1.0|                      0.625|        0.5594133492965331|                        0.0|                        0.0|        0.1822933681803328|\n",
      "|     1|         2| 641.71|1588.45|1395.42| 554.85|9054.42|  522.16| 8139.62|   393.0| 14|             1|             9052.295|              1586.87|                 392.5|              642.365|             1396.815|              554.375|              8132.585|                521.94|        0.8640350877193341|        0.7292294446835668|        0.2802197802198749|                      0.875|        0.8138281951510964|         0.9154196486661663|         0.5207100591717855|                       0.0|\n",
      "|     1|         3| 642.46|1586.94|1401.34| 554.11|9056.96|  521.97|  8130.1|   393.0| 13|             1|              9053.85|   1586.8933333333334|     392.6666666666667|    642.3966666666666|   1398.3233333333335|    554.2866666666667|     8131.756666666667|                521.95|        0.7090643274855706|        0.7332472377672583|       0.31501831501837496|         0.9583333333333428|                       1.0|         0.8076339188896045|          0.544378698225024|       0.19710334313406802|\n",
      "+------+----------+-------+-------+-------+-------+-------+--------+--------+--------+---+--------------+---------------------+---------------------+----------------------+---------------------+---------------------+---------------------+----------------------+----------------------+--------------------------+--------------------------+--------------------------+---------------------------+--------------------------+---------------------------+---------------------------+--------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialiser la session Spark\n",
    "spark = SparkSession.builder.appName(\"Entrainement\").getOrCreate()\n",
    "\n",
    "# Charger les données\n",
    "train_df = spark.read.csv(\"data/train_preprocessed.csv\", header=True, inferSchema=True)\n",
    "test_df = spark.read.csv(\"data/test_preprocessed.csv\", header=True, inferSchema=True)\n",
    "test_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#engines_list = train_df.select(\"engine\").distinct().rdd.map(lambda row: row[\"engine\"]).collect()\n",
    "\n",
    "\n",
    "engines_list = [18, 30, 38, 12, 34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df.select(train_df[ \"sensor12_rollingmean_4_norm\"],train_df[\"sensor7_rollingmean_4_norm\"], train_df[\"rul\"])\n",
    "\n",
    "train_vector = train.rdd.map(lambda x: [x[2], Vectors.dense(x[0:2])]).toDF(['label','features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Régression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Coefficients: [151.98148729693617,91.21112385852294]\n",
      "Engine #  18\n",
      "average R-Squared:  0.6233644519732792\n",
      "average MSE:  1786.8949427233879\n",
      "average RMSE:  42.27168015023046\n",
      "average MAE:  33.08518173893914\n",
      "Engine #  30\n",
      "average R-Squared:  0.6233644519732792\n",
      "average MSE:  1786.8949427233879\n",
      "average RMSE:  42.27168015023046\n",
      "average MAE:  33.08518173893914\n",
      "Engine #  38\n",
      "average R-Squared:  0.6233644519732792\n",
      "average MSE:  1786.894942723388\n",
      "average RMSE:  42.27168015023046\n",
      "average MAE:  33.08518173893914\n",
      "Engine #  12\n",
      "average R-Squared:  0.6233644519732792\n",
      "average MSE:  1786.8949427233879\n",
      "average RMSE:  42.27168015023046\n",
      "average MAE:  33.08518173893914\n",
      "Engine #  34\n",
      "average R-Squared:  0.6233644519732792\n",
      "average MSE:  1786.8949427233879\n",
      "average RMSE:  42.27168015023046\n",
      "average MAE:  33.08518173893914\n",
      "CPU times: total: 375 ms\n",
      "Wall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "regression(train_vector,test_df,engines_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression par Arbre de Décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median R-Squared:  0.6577866056429873\n",
      "median MSE:  1623.5838250333366\n",
      "median RMSE:  40.29371942416506\n",
      "median MAE:  31.409797763333252\n",
      "median R-Squared:  0.6749228215291734\n",
      "median MSE:  1542.2834335411624\n",
      "median RMSE:  39.27191660132164\n",
      "median MAE:  30.55878217728309\n",
      "median R-Squared:  0.6577866056429873\n",
      "median MSE:  1623.5838250333366\n",
      "median RMSE:  40.29371942416506\n",
      "median MAE:  31.409797763333252\n",
      "median R-Squared:  0.6778772907985126\n",
      "median MSE:  1528.2663652546623\n",
      "median RMSE:  39.09304753091862\n",
      "median MAE:  30.37980595845084\n",
      "CPU times: total: 1.53 s\n",
      "Wall time: 11min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from ArbreDecision import Arbre_de_decision\n",
    "\n",
    "Arbre_de_decision(train_vector,test_df,engines_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Binaire "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import  DoubleType\n",
    "\n",
    "\n",
    "engines_list = train_df.select(\"engine\").distinct().rdd.map(lambda row: row[\"engine\"]).collect()\n",
    "\n",
    "train = train_df.select(train_df[\"sensor3_rollingmean_4_norm\"],train_df[\"sensor4_rollingmean_4_norm\"],\n",
    "                        train_df[\"sensor7_rollingmean_4_norm\"],\n",
    "                        train_df[\"sensor12_rollingmean_4_norm\"],\n",
    "                        train_df[\"sensor17_rollingmean_4_norm\"],train_df[\"classification\"])\n",
    "\n",
    "train_vector = train.rdd.map(lambda x: [x[5], Vectors.dense(x[0:5])]).toDF(['label','features'])\n",
    "train_vector = train_vector.withColumn('label', train_vector['label'].cast(DoubleType()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistique Regression:\n",
      " {'Accuracy': {'mean': nan}, 'Macro Precision': {'mean': 0.824}, 'Macro Recall': {'mean': 0.832}, 'Macro F1': {'mean': 0.828}}\n",
      "CPU times: total: 4.59 s\n",
      "Wall time: 2min 35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Slash\\anaconda3\\envs\\new_pyspark_env\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Users\\Slash\\anaconda3\\envs\\new_pyspark_env\\lib\\site-packages\\numpy\\core\\_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from LogisticRegression import LogsticRegression\n",
    "LogsticRegression(train_vector,test_df,engines_list[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbre de decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree: \n",
      " {'Accuracy': {0.851}, 'Macro Precision': {0.85}, 'Macro Recall': {0.8299999999999998}, 'Macro F1': {0.8400000000000001}}\n",
      "CPU times: total: 7.86 s\n",
      "Wall time: 4min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from DecisionTree_classification import DecsionTree\n",
    "\n",
    "DecsionTree(train_vector,test_df,engines_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest: \n",
      " {'Accuracy': {0.861}, 'Macro Precision': {0.852}, 'Macro Recall': {0.8400000000000001}, 'Macro F1': {0.8459999999999999}}\n",
      "CPU times: total: 8.62 s\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from RandomForest import RandomForest\n",
    "\n",
    "RandomForest(train_vector,test_df,engines_list[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
